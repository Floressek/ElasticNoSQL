# ğŸª ELK Stack - System Analityki SprzedaÅ¼y

Kompletny system do zbierania, przetwarzania i analizowania danych sprzedaÅ¼owych w czasie rzeczywistym oparty na stosie ELK (Elasticsearch, Logstash, Kibana) z Flask API jako generatorem danych.

## ğŸ“‹ Spis treÅ›ci

- [Architektura systemu](#-architektura-systemu)
- [FunkcjonalnoÅ›ci](#-funkcjonalnoÅ›ci)
- [Wymagania](#-wymagania)
- [Instalacja i uruchomienie](#-instalacja-i-uruchomienie)
- [Konfiguracja](#-konfiguracja)
- [API Endpoints](#-api-endpoints)
- [Struktura danych](#-struktura-danych)
- [Monitorowanie](#-monitorowanie)
- [RozwiÄ…zywanie problemÃ³w](#-rozwiÄ…zywanie-problemÃ³w)

## ğŸ— Architektura systemu

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    HTTP    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Flask API   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  Logstash    â”‚â”€â”€â”€â”€â–¶â”‚ Elasticsearch   â”‚â”€â”€â”€â”€â–¶â”‚   Kibana    â”‚
â”‚ Generator   â”‚    :8080    â”‚  Pipeline    â”‚     â”‚    Cluster      â”‚     â”‚ Dashboards  â”‚
â”‚ SprzedaÅ¼y   â”‚             â”‚              â”‚     â”‚                 â”‚     â”‚             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     :5001                         :8080              :9200/:9300              :5601
```

### Komponenty:

- **ğŸ Flask API** - Generator realistycznych transakcji sprzedaÅ¼owych
- **ğŸ“Š Logstash** - Przetwarzanie i wzbogacanie danych w czasie rzeczywistym
- **ğŸ” Elasticsearch** - Klaster 5 wÄ™zÅ‚Ã³w (3 master + 2 data) do przechowywania danych
- **ğŸ“ˆ Kibana** - Interface do wizualizacji i analizy danych

## âœ¨ FunkcjonalnoÅ›ci

### Generator danych (Flask API)
- **15 realistycznych produktÃ³w** - Apple, Samsung, Sony, Nintendo, itp.
- **7 lokalizacji sklepÃ³w** - Warszawa, KrakÃ³w, GdaÅ„sk, WrocÅ‚aw, PoznaÅ„, ÅÃ³dÅº, Katowice
- **RÃ³Å¼norodne transakcje** - rÃ³Å¼ne metody pÅ‚atnoÅ›ci, rabaty, klienci
- **Automatyczne generowanie** - moÅ¼liwoÅ›Ä‡ ciÄ…gÅ‚ego generowania danych

### Przetwarzanie danych (Logstash)
- **Klasyfikacja transakcji** - basic/standard/premium wedÅ‚ug wartoÅ›ci
- **Wzbogacanie geolokalizacyjne** - lokalizacja klientÃ³w na podstawie IP
- **Obliczanie marÅ¼y** - automatyczne wyliczanie marÅ¼y wedÅ‚ug kategorii produktu
- **System alertÃ³w** - powiadomienia dla transakcji powyÅ¼ej 8000 PLN
- **Tagowanie kategorii** - automatyczne oznaczanie produktÃ³w high-value

### Analityka (Elasticsearch + Kibana)
- **Indeksowanie dzienne** - automatyczne tworzenie indeksÃ³w `sprzedaz-YYYY.MM.DD`
- **Indeksy alertÃ³w** - oddzielne przechowywanie wysokich transakcji
- **Struktura ECS** - zgodnoÅ›Ä‡ z Elastic Common Schema
- **Dashboardy** - gotowe do tworzenia wizualizacji w Kibana

## ğŸ”§ Wymagania

- **Docker** 20.10+
- **Docker Compose** 2.0+
- **8GB RAM** (rekomendowane dla Elasticsearch)
- **Porty:** 5001, 5601, 8080, 9200, 9300

## ğŸš€ Instalacja i uruchomienie

### 1. Klonowanie repozytorium
```bash
git clone <repository-url>
cd ElasticNoSQL
```

### 2. Uruchomienie systemu
```bash
# Uruchomienie wszystkich kontenerÃ³w
docker-compose up -d

# Sprawdzenie statusu
docker-compose ps
```

### 3. Weryfikacja uruchomienia
```bash
# SprawdÅº czy Elasticsearch dziaÅ‚a
curl http://localhost:9200/_cluster/health?pretty

# SprawdÅº Flask API
curl http://localhost:5001/health

# SprawdÅº Kibana (w przeglÄ…darce)
http://localhost:5601
```

### 4. Generowanie pierwszych danych
```bash
# Wygeneruj 10 transakcji testowych
curl http://localhost:5001/generate/10

# Lub uruchom automatyczne generowanie (20 transakcji co 5 sekund)
curl http://localhost:5001/auto-generate/20
```

## âš™ï¸ Konfiguracja

### Zmienne Å›rodowiskowe

| Zmienna | WartoÅ›Ä‡ domyÅ›lna | Opis |
|---------|------------------|------|
| `ELASTICSEARCH_HOST` | `es-data-1` | Host Elasticsearch |
| `LOGSTASH_HOST` | `logstash` | Host Logstash |
| `ES_JAVA_OPTS` | `-Xms1g -Xmx1g` | Ustawienia JVM dla ES |

### Konfiguracja Elasticsearch

```yaml
# docker-compose.yml
environment:
  - cluster.name=sklep-cluster
  - node.roles=master  # lub data,ingest
  - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
  - xpack.security.enabled=false
```

### Konfiguracja Logstash

Pipeline znajduje siÄ™ w `logstash/pipeline/logstash.conf` i zawiera:
- Input HTTP na porcie 8080
- Filtry do wzbogacania danych
- Output do Elasticsearch

## ğŸ”Œ API Endpoints

### Flask API (port 5001)

| Endpoint | Metoda | Opis | PrzykÅ‚ad |
|----------|--------|------|----------|
| `/` | GET | Strona gÅ‚Ã³wna z informacjami | `GET /` |
| `/health` | GET | Status zdrowia serwera | `GET /health` |
| `/generate/<liczba>` | GET | Generuje N transakcji | `GET /generate/10` |
| `/auto-generate/<liczba>` | GET | Auto-generowanie co 5s | `GET /auto-generate/20` |
| `/stats` | GET | Statystyki produktÃ³w | `GET /stats` |

### PrzykÅ‚ady uÅ¼ycia

```bash
# Podstawowe informacje
curl http://localhost:5001/

# Wygeneruj 5 transakcji
curl http://localhost:5001/generate/5

# Statystyki produktÃ³w i kategorii
curl http://localhost:5001/stats

# Auto-generowanie 50 transakcji (co 5 sekund)
curl http://localhost:5001/auto-generate/50
```

## ğŸ“Š Struktura danych

### PrzykÅ‚adowa transakcja

```json
{
  "timestamp": "2025-06-08T12:30:45.123456",
  "transaction_id": "TXN_20250608_12345",
  "level": "INFO",
  "service": "sklep-pos",
  "action": "sprzedaz",
  "message": "SprzedaÅ¼ produktu iPhone 15 Pro",
  
  "product_id": "PHONE001",
  "product_name": "iPhone 15 Pro",
  "product_category": "telefony",
  "product_brand": "Apple",
  "unit_price": 5499.99,
  "quantity": 1,
  
  "amount_gross": 5499.99,
  "discount_percent": 10,
  "discount_amount": 549.99,
  "amount_net": 4950.00,
  "currency": "PLN",
  "payment_method": "karta",
  
  "store_city": "Warszawa",
  "store_code": "WAW",
  "store_location": {
    "lat": 52.2297,
    "lon": 21.0122
  },
  
  "customer_name": "Jan Kowalski",
  "customer_ip": "192.168.1.100",
  "session_id": "sess_123456",
  "employee_id": "EMP1025",
  "receipt_number": "RC_20250608_5432"
}
```

### Pola dodawane przez Logstash

```json
{
  "transaction_class": "premium",  // basic/standard/premium
  "priority": "high",              // low/medium/high
  "margin_percent": 20,            // % marÅ¼y wedÅ‚ug kategorii
  "margin_amount": 990.00,         // kwota marÅ¼y
  "business_hour": "12",           // godzina transakcji
  "data_source": "flask_api",      // ÅºrÃ³dÅ‚o danych
  "processed_by": "logstash",      // przetworzony przez
  "environment": "development"     // Å›rodowisko
}
```

## ğŸ“ˆ Monitorowanie

### Logi systemu

```bash
# Wszystkie logi
docker-compose logs

# Logi konkretnego serwisu
docker-compose logs -f flask-api
docker-compose logs -f logstash
docker-compose logs -f es-master-1

# Logi w czasie rzeczywistym
docker-compose logs -f
```

### Elasticsearch API

```bash
# Status klastra
curl http://localhost:9200/_cluster/health?pretty

# Lista indeksÃ³w
curl http://localhost:9200/_cat/indices?v

# Wyszukiwanie w danych sprzedaÅ¼y
curl "http://localhost:9200/sprzedaz-*/_search?pretty&size=5"

# Statystyki indeksÃ³w
curl http://localhost:9200/_cat/indices/sprzedaz-*?v&h=index,docs.count,store.size
```

### Kibana Dashboards

1. OtwÃ³rz http://localhost:5601
2. PrzejdÅº do `Stack Management` â†’ `Index Patterns`
3. UtwÃ³rz pattern `sprzedaz-*` z polem czasowym `@timestamp`
4. PrzejdÅº do `Discover` aby przeglÄ…daÄ‡ dane
5. UtwÃ³rz dashboardy w `Dashboard`

## ğŸ”§ RozwiÄ…zywanie problemÃ³w

### Problem: Flask API nie odpowiada

```bash
# SprawdÅº status kontenera
docker-compose ps flask-api

# SprawdÅº logi
docker-compose logs flask-api

# Restart serwisu
docker-compose restart flask-api
```

### Problem: Elasticsearch nie uruchamia siÄ™

```bash
# ZwiÄ™ksz vm.max_map_count (Linux)
sudo sysctl -w vm.max_map_count=262144

# Windows/Docker Desktop - zwiÄ™ksz pamiÄ™Ä‡ do 4GB+
```

### Problem: Brak danych w Elasticsearch

```bash
# SprawdÅº czy Logstash otrzymuje dane
docker-compose logs logstash | grep "flask-api"

# SprawdÅº indeksy
curl http://localhost:9200/_cat/indices?v

# Test poÅ‚Ä…czenia Logstash
curl -X POST http://localhost:8080 -H "Content-Type: application/json" -d '{"test": "message"}'
```

### Problem: Kibana nie Å‚Ä…czy siÄ™ z Elasticsearch

```bash
# SprawdÅº logi Kibana
docker-compose logs kibana

# SprawdÅº dostÄ™pnoÅ›Ä‡ ES z kontenera Kibana
docker-compose exec kibana curl http://es-data-1:9200
```

## ğŸ“ Struktura projektu

```
ElasticNoSQL/
â”œâ”€â”€ docker-compose.yml          # GÅ‚Ã³wny plik Docker Compose
â”œâ”€â”€ Dockerfile                  # Dockerfile dla Flask API
â”œâ”€â”€ server.py                   # Flask API - generator danych
â”œâ”€â”€ requirements.txt            # ZaleÅ¼noÅ›ci Python (opcjonalne)
â”œâ”€â”€ .gitignore                  # Pliki ignorowane przez git
â”œâ”€â”€ data/
â”‚   â””â”€â”€ logs/
â”‚       â””â”€â”€ .gitkeep           # Katalog na logi
â””â”€â”€ logstash/
    â”œâ”€â”€ config/
    â”‚   â”œâ”€â”€ logstash.yml       # Konfiguracja Logstash
    â”‚   â”œâ”€â”€ pipelines.yml      # Definicja pipeline
    â”‚   â””â”€â”€ sprzedaz-template.json  # Template ES
    â””â”€â”€ pipeline/
        â””â”€â”€ logstash.conf      # Pipeline przetwarzania
```

### Dodawanie nowych filtrÃ³w Logstash

Edytuj `logstash/pipeline/logstash.conf` w sekcji `filter`.

### Modyfikacja klastra Elasticsearch

Dodaj nowe wÄ™zÅ‚y w `docker-compose.yml` lub zmieÅ„ konfiguracjÄ™ istniejÄ…cych.

## ğŸ“„ Licencja

MIT License - szczegÃ³Å‚y w pliku LICENSE.

---

**Autor:** floressek  
**Wersja:** 1.0  
**Data:** 2025-06-08
